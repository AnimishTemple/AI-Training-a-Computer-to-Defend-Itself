{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SORb0LJ3SVe6"
      },
      "source": [
        "#Obtain the dataset from the GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1Tt7knbfUgI",
        "outputId": "ca6f525d-d6fa-4ca0-f9cf-99485e809f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DikeDataset'...\n",
            "remote: Enumerating objects: 11973, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 11973 (delta 6), reused 23 (delta 4), pack-reused 11946\u001b[K\n",
            "Receiving objects: 100% (11973/11973), 2.38 GiB | 17.42 MiB/s, done.\n",
            "Resolving deltas: 100% (2739/2739), done.\n",
            "Checking out files: 100% (11935/11935), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/iosifache/DikeDataset.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig9y1JSDSf5C"
      },
      "source": [
        "#Import the necessary modules and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vsBDVFuv9vg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiiXRtN7iRjb"
      },
      "source": [
        "#Reading integers from the bytes in each file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoEOWyHZspty"
      },
      "outputs": [],
      "source": [
        "def read_ints_from_file(file_name, file_size):\n",
        "  image_numpy_arr = np.zeros((256, 256), dtype=int)\n",
        "  max_file_size = 256*256\n",
        "  curr_file = open(file_name, \"rb\")\n",
        "  bytes_read = 0\n",
        "  row_num = 0\n",
        "  col_num = 0\n",
        "  try:\n",
        "    byte = curr_file.read(1)\n",
        "    curr_int = int.from_bytes(byte, \"little\")\n",
        "    bytes_read += 1\n",
        "    image_numpy_arr[row_num][col_num] = curr_int\n",
        "    while bytes_read <= file_size and bytes_read < max_file_size and row_num < 255:\n",
        "      byte = curr_file.read(1)\n",
        "      curr_int = int.from_bytes(byte, \"little\")\n",
        "      if col_num < 255:\n",
        "        col_num += 1\n",
        "        image_numpy_arr[row_num][col_num] = curr_int\n",
        "      else:\n",
        "        row_num += 1\n",
        "        col_num = 0\n",
        "        image_numpy_arr[row_num][col_num] = curr_int\n",
        "  finally:\n",
        "    curr_file.close()\n",
        "    return image_numpy_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58fdl84B5-pe"
      },
      "source": [
        "# Generating the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OKwYARziWBB"
      },
      "outputs": [],
      "source": [
        "# will only use 1,000 of each file to save time and space\n",
        "def generate_training_data():\n",
        "  training_data = []\n",
        "  # 0 for benign files, 1 for malware files\n",
        "  classifications = [\"benign\", \"malware\"]\n",
        "  sample_files_dir = \"DikeDataset/files\"\n",
        "  for classification in classifications:\n",
        "    path = os.path.join(sample_files_dir, classification)\n",
        "    label_num = classifications.index(classification)\n",
        "    for file in os.listdir(path)[:1000]:\n",
        "      file_name = os.path.join(path, file)\n",
        "      file_size = os.path.getsize(file_name)\n",
        "      img_array = read_ints_from_file(file_name, file_size)\n",
        "      training_data.append([img_array, label_num])\n",
        "  return training_data\n",
        "\n",
        "# shuffle the training data so the neural network can effectively learn\n",
        "training_data = generate_training_data()\n",
        "random.shuffle(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_YA_B2W6f3m"
      },
      "source": [
        "# Extracting the features and labels from training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71AH_PV0XZL8"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "for feature, label in training_data:\n",
        "  X.append(feature)\n",
        "  y.append(label)\n",
        "\n",
        "# reshape the data before entering the neural network\n",
        "X = np.array(X).reshape(-1, 256, 256, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeuJsshV63X1"
      },
      "source": [
        "# Use the Pickle library to store and load your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHd9kK4Ig7pZ"
      },
      "outputs": [],
      "source": [
        "pickle_file = open(\"X.pickle\", \"wb\")\n",
        "pickle.dump(X, pickle_file)\n",
        "pickle_file.close()\n",
        "\n",
        "pickle_file = open(\"y.pickle\", \"wb\")\n",
        "pickle.dump(y, pickle_file)\n",
        "pickle_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R94HVW1AheJb"
      },
      "outputs": [],
      "source": [
        "pickle_read = open(\"X.pickle\", \"rb\")\n",
        "X = pickle.load(pickle_read)\n",
        "\n",
        "pickle_read = open(\"y.pickle\", \"rb\")\n",
        "y = pickle.load(pickle_read)\n",
        "\n",
        "X = X/255\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "101QLW6_69z2"
      },
      "source": [
        "# Training the neural network to recognize malware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2gU-p_RjH6e",
        "outputId": "545f3619-2185-4826-8156-ea38a16bc9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "90/90 [==============================] - 6s 57ms/step - loss: 1.0840 - accuracy: 0.8167 - val_loss: 0.1896 - val_accuracy: 0.9600\n",
            "Epoch 2/10\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.1587 - accuracy: 0.9467 - val_loss: 0.1657 - val_accuracy: 0.9300\n",
            "Epoch 3/10\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.0939 - accuracy: 0.9711 - val_loss: 0.1085 - val_accuracy: 0.9650\n",
            "Epoch 4/10\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.0447 - accuracy: 0.9867 - val_loss: 0.1067 - val_accuracy: 0.9700\n",
            "Epoch 5/10\n",
            "90/90 [==============================] - 5s 56ms/step - loss: 0.0213 - accuracy: 0.9961 - val_loss: 0.1187 - val_accuracy: 0.9700\n",
            "Epoch 6/10\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.1282 - val_accuracy: 0.9600\n",
            "Epoch 7/10\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.0105 - accuracy: 0.9983 - val_loss: 0.1423 - val_accuracy: 0.9700\n",
            "Epoch 8/10\n",
            "90/90 [==============================] - 5s 56ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9700\n",
            "Epoch 9/10\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9700\n",
            "Epoch 10/10\n",
            "90/90 [==============================] - 5s 56ms/step - loss: 8.4176e-04 - accuracy: 1.0000 - val_loss: 0.1755 - val_accuracy: 0.9700\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe7001641d0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X, y, batch_size=20, epochs=10, validation_split=0.1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}